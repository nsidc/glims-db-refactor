2024-05-23

Created glacier entity object, so now need to redo the tests.


2024-05-30

Parent RCs must be in the DB before the children.  A sort on rc_id takes
care of this.


2024-06-10

After copying database, I think the autoincrement indices will need to be
reset with ALTER SEQUENCE.  See
https://www.postgresql.org/docs/current/sql-altersequence.html


2024-07-17

Cases involving multi-part polygons could be complex.  Here are some I can
think of:

1.  Simple:  A small number of parts.  Each part gets its own GLIMS glacier
    ID, possibly reusing the one that was assigned to the group.  Would
    then need to (pseudocode):

    for p in parts:
        - Give p a new GLIMS glacier ID,
        - Put new entry in glacier_static for this "new" outline,
        - Combine any intrnl_rock outlines contained by p as a hole,
        - Write to glacier_dynamic and glacier_entities

2.  Other entities:  Same as above, but there are debris-cover, lake, etc.
    outlines that have the same glacier ID as the group.  Then need to
    (copying some steps from above):

    for p in parts:
        - Give p a new GLIMS glacier ID,
        - Put new entry in glacier_static for this "new" outline,
        - Combine any intrnl_rock outlines contained by p as a hole,
        - Write to glacier_dynamic and glacier_entities
        - Go through all other entities (in whole input data set?  Just
          those tagged with the group glacier ID?), and for each entity
          contained in p, assign the entity the same new glacier ID, then
        - write to the glacier_entities table

These can be combined into one:

    for p in parts:
        Give p a new GLIMS glacier ID -> g_id.
        Determine what analysis_id p will have -> a_id.

        for n in intrnl_rock outlines with this group ID:
            If n is contained by p, combine n into p as a "hole".

        for e in all other entities (with same ID? or whole data set? Latter is slower but safer.):
            If e is contained in p, assign g_id and a_id to e.

        Write new entry in glacier_static for this "new" outline p.
        Write "holey" p, [n, ...], and [e, ...] to the glacier_entities table (with g_id, a_id).


2024-10-22

The counts of INSERT statements and rows in the old DB match, except for the
new glacier_entities table, which hasn't been fully populated yet.


2024-10-31

Where are the uncertainty values in the new schema?  They are in the segments
table in the old, and should therefore be in the glacier_entities table in the
new.

Also, for the download service to work, I need to remove all reference to
segments, lines, etc.

There are currently polygon, line, and point queries, the results of which end
up in different shapefiles.  So, I need to define new views for these that
include something like "WHERE glacier_entities.line_type=<geomtype>".


2024-11-27

To do this move, I think I should do:

1.  simple row-by-row copy up through submission_info;
2.  step through the glacier_dynamic and glacier_static tables and analyze the
    outlines together, break up multipolygons, then write what survives or gets
    created to those three tables;
3.  write the remaining tables, saving those possibly new or changed primary
    keys (glacier_id and analysis_id values).


2024-11-29

The run_on_iceland.sh process seems to have finished normally.  Took 45.53 hours.  Needs testing.

- Download data (reprovision services first) and visualize


2024-12-03

Geometry is still messed up.  Something is wrong in the translation from the
shapely representation to the form I'm giving PostGIS.  Will first try pgsql2shp.

Transfer of polygons from old to new DBs apparently is dropping the largest
polygon encompassing all of Vatnajokull.  The geometry problems, however, might
be in the new download script, since I don't see problems in the shapefile from
the pgsql2shp script.

So, I need the transfer script to be less sensitive to topology errors.  The
approach taken in the make_valid_if_possible routine is not the right one.


2025-01-13

The current version seems to fix topology, and also explodes glacier boundary
polygon lists and multi-polygons.  I think I need to do the same for the
internal rock polygons.


2025-01-15

Ran the move script with the new "explode_multipolygons" function applied to
the glac_bound polygons, but not the internal rock polygons.  Results look the
same:  great when using pgsql2shp, but connecting lines when using the download
function.

Will now truncate the DB and run again using the version of the move script the
applies the explode_multipolygons function to the internal rocks also.

I see that some inserts into glacier_static are failing due to duplicate
glacier IDs.  So, the process of assigning new glacier IDs to parts from
multipolygons is not doing adequate checking for already-used IDs.


2025-02-13

Trying to find out how many GLIMS database entries are multi-polygon, but this doesn't do it:

glims=# select left(st_astext(glacier_polys), 10) as geom_type, count(*) from glacier_polygons group by geom_type ;
 geom_type  |  count  
 ------------+---------
  POLYGON Z  | 1319217
  (1 row)


2025-02-19

Testing the new (correct) way to assign glacier IDs to parts split out from
multi-polygons.  Added spatial indexing to (hopefully) speed that up a bit.


2025-02-20

Current status:  The ID assignment, I think, it working okay now, but the
make_valid routine to fix topology errors creates multi-polygons, which are
getting discarded.  This is throwing away two many polygons.  See comments in
the make_valid_poly_if_possible routine.
